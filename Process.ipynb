{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,os,time\n",
    "import spacy\n",
    "import csv\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring,qparser\n",
    "from whoosh.index import open_dir\n",
    "from whoosh.collectors import TimeLimitCollector, TimeLimit\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp_large = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "These functions are used to extract key information in the claim to make it as a query.\n",
    "'''\n",
    "def spacy_ner(string):\n",
    "        doc = nlp(string)\n",
    "        result = []\n",
    "        for w in doc.ents:\n",
    "            if w.label_ in ['PERSON']:\n",
    "                for token in w:\n",
    "                    result.append(str(token))\n",
    "            elif w.label_ in ['LOC','EVENT','WORK_OF_ART', 'NORP', 'FAC', 'ORG', 'GPE', 'LAW', 'LANGUAGE', 'PRODUCT']:\n",
    "                text = ''\n",
    "                for token in w:\n",
    "                    if not(token.is_stop) and not(token.is_punct):\n",
    "                        text = ' '.join([text,str(token)]).strip()\n",
    "                result.append(text)\n",
    "        #result = [(w.text, w.label_) for w in doc.ents]\n",
    "        return result\n",
    "    \n",
    "def spacy_pos(string):\n",
    "    doc = nlp(string)\n",
    "    result = []\n",
    "    text = ''\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['PROPN', 'NOUN'] and not(token.is_stop) and not (token.is_punct):\n",
    "            if token.shape_[0] == 'X':#begin with uppercase\n",
    "                if text != '':\n",
    "                    result.append(text)\n",
    "                    text  = str(token)\n",
    "                else:\n",
    "                    text = (text+' '+str(token)).strip()\n",
    "            else:\n",
    "                text = (text+' '+str(token)).strip()\n",
    "        else:\n",
    "            if text != '':\n",
    "                result.append(text)\n",
    "                text = ''\n",
    "    if text != '':\n",
    "        result.append(text)\n",
    "    return result\n",
    "    \n",
    "def spacy_chunk(string):\n",
    "    doc = nlp(string)\n",
    "    result = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        text = ''\n",
    "        for token in chunk:\n",
    "            if not(token.is_stop) and not(token.is_punct):\n",
    "                if token.shape_[0] == 'X':\n",
    "                    if text != '':\n",
    "                        result.append(text)\n",
    "                        text = str(token)\n",
    "                    else:\n",
    "                        text = (text+' '+str(token)).strip()\n",
    "                else:\n",
    "                    text = (text+' '+str(token)).strip()\n",
    "        if text != '':\n",
    "            result.append(text)\n",
    "    return result\n",
    "\n",
    "def spacy_pos_single(string):\n",
    "    doc = nlp(string)\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['PROPN', 'NOUN'] and token.shape_[0] == 'X' and not(token.is_stop) and not (token.is_punct):\n",
    "            result.append(token.text)\n",
    "    return result\n",
    "\n",
    "def spacy_pos_final(string):\n",
    "    doc = nlp(string)\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['ADJ','INTJ','NUM','VERB','PRON'] and token.shape_[0] == 'X' and not(token.is_stop) and not (token.is_punct):\n",
    "            result.append(token.text)\n",
    "        if token.pos_ in ['PROPN', 'NOUN'] and not(token.is_stop) and not (token.is_punct):\n",
    "            result.append(token.text)\n",
    "    return result\n",
    "\n",
    "def spacy_chunk_single(string):\n",
    "    doc = nlp(string)\n",
    "    result = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        for token in chunk:\n",
    "            if not(token.is_stop) and not(token.is_punct):\n",
    "                result.append(str(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For solving encoding issue in this project\n",
    "import unicodedata\n",
    "\n",
    "def nfd(string):\n",
    "    return unicodedata.normalize('NFD',string)\n",
    "\n",
    "def nfc(string):\n",
    "    return unicodedata.normalize('NFC',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A method to calculate the similarity between two sentences\n",
    "#spacy similarity\n",
    "def similarity_spacy_large(sentence1,sentence2):\n",
    "    doc1 = nlp_large(sentence1)\n",
    "    doc2 = nlp_large(sentence2)\n",
    "    return doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A method to calculate the similarity between two sentences\n",
    "#gensim - word2vec - average weight\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.linalg import norm\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def vector_similarity(claim, sentence):\n",
    "    text_dict = [sentence.lower().split()+claim.lower().split()]\n",
    "    model = Word2Vec(min_count=1)\n",
    "    model.build_vocab(text_dict)  # prepare the model vocabulary\n",
    "    model.train(text_dict, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
    "    \n",
    "    def sentence_vector(s):\n",
    "        words = s.lower().split()\n",
    "        v = np.zeros(100)\n",
    "        for word in words:\n",
    "            v += model.mv[word]\n",
    "        v /= len(words)\n",
    "        return v\n",
    "    \n",
    "    v1, v2 = sentence_vector(claim), sentence_vector(sentence)\n",
    "    return np.dot(v1, v2) / (norm(v1) * norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A method to calculate the similarity between two sentences\n",
    "#gensim - word2vec -cosine\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def cosine_similarity(claim, sentence):\n",
    "    text_dict = [sentence.lower().split()+claim.lower().split()]\n",
    "    model = Word2Vec(min_count=1)\n",
    "    model.build_vocab(text_dict)  # prepare the model vocabulary\n",
    "    model.train(text_dict, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
    "    \n",
    "    vector_1 = np.mean([model.mv[word] for word in claim.lower().split()],axis=0)\n",
    "    vector_2 = np.mean([model.mv[word] for word in sentence.lower().split()],axis=0)\n",
    "    cosine = scipy.spatial.distance.cosine(vector_1, vector_2)\n",
    "    return (1-cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A method to rank the sentences by the similarity with the claim\n",
    "#gensim - lsi\n",
    "#many evidence input to text_dict\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim import models\n",
    "\n",
    "def gensim_lsi(claim,sentences):\n",
    "    text_dict = [sentence.lower().split() for sentence in sentences]\n",
    "    #print(text_dict)\n",
    "    dictionary = corpora.Dictionary(text_dict)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_dict]\n",
    "    tfidf = models.TfidfModel(corpus)#[corpus]\n",
    "    lsi_model = models.LsiModel(corpus, id2word=dictionary,num_topics=2)\n",
    "    claim_bow = dictionary.doc2bow(claim.lower().split())    \n",
    "    query_vec = lsi_model[claim_bow]\n",
    "    documents = lsi_model[corpus]\n",
    "    index = similarities.MatrixSimilarity(documents)\n",
    "    sims = index[query_vec]\n",
    "    result = list(enumerate(sims)) # (document_number, document_similarity) 2-tuples\n",
    "    result.sort(key=lambda x:x[1],reverse=True)\n",
    "    return [result[i][0] for i in range(len(result))] # print sorted document_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A dumb function that searches the topN documents by the input claim.\n",
    "return a dict in form: value = {\"claim\":claim, \"label\":\"SUPPORTS\", \"evidence\":evidence}\n",
    "where the evidence is a list, in which every doc's first sentence is taken.\n",
    "transformer is the method name for extracting informantion. e.g. spacy_pos\n",
    "'''\n",
    "def search_by_query_list(claim, transformer, topN, parser, searcher):\n",
    "    query_str_list = transformer(claim)\n",
    "    \n",
    "    if len(query_str_list) == 0:\n",
    "        value = {\"claim\":claim, \"label\":\"NOT ENOUGH INFO\", \"evidence\":[]}\n",
    "        return value\n",
    "    \n",
    "    # query_str_list to query_str\n",
    "    query_str = \"\\'\"+query_str_list[0]+\"\\'\"        \n",
    "    for i in range(len(query_str_list)-1):\n",
    "        query_str += \" OR \\'\"+ query_str_list[i+1] +\"\\'\"\n",
    "        \n",
    "    query = parser.parse(query_str)\n",
    "    evidence = []\n",
    "    \n",
    "    #search with collector\n",
    "    my_collector = searcher.collector(limit=topN,optimize=False)\n",
    "    tlc = TimeLimitCollector(my_collector, timelimit=30)\n",
    "    try:\n",
    "        searcher.search_with_collector(query,tlc)\n",
    "    except TimeLimit:\n",
    "        pass\n",
    "    results=tlc.results()\n",
    "            \n",
    "    for i in range(min(topN,len(results))):\n",
    "        doc = results[i]['content']\n",
    "        sentence = doc.split(\"\\n\")[0]\n",
    "        number = int(sentence.split()[1])\n",
    "        title = unicodedata.normalize('NFD',results[i]['title'])\n",
    "        evidence.append([title,number])\n",
    "    value = {\"claim\":claim, \"label\":\"SUPPORTS\", \"evidence\":evidence}\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The function searches the top N documents for document recall test\n",
    "Input is json file containing claim\n",
    "Output file is a corresponding json in standard format.\n",
    "'''\n",
    "def find_doc(openfile, savefile, transfomer, topN):\n",
    "    ix = open_dir(\"indexdir\")\n",
    "    searcher = ix.searcher(weighting=scoring.BM25F(B=0.75, K1=1.2))\n",
    "    parser = QueryParser(None, ix.schema) \n",
    "    parser.add_plugin(qparser.MultifieldPlugin([\"content\"]))\n",
    "    \n",
    "    with open(openfile,'r') as f:\n",
    "        with open(savefile,\"w\",encoding='ascii') as dump_f:\n",
    "            dump_f.write('{')\n",
    "            load_dict = json.load(f)\n",
    "            for key,val in load_dict.items():\n",
    "                claim_number = key\n",
    "                claim = val['claim']\n",
    "\n",
    "                #search by claim\n",
    "                value = search_by_query_list(claim, transfomer, topN, parser, searcher)\n",
    "\n",
    "                dump_f.write(json.dumps(claim_number)+':'+json.dumps(value))\n",
    "                dump_f.write(',')\n",
    "                dump_f.write('\\n')\n",
    "\n",
    "        with open(savefile,\"rb+\") as filehandler:\n",
    "            filehandler.seek(-3, os.SEEK_END)\n",
    "            filehandler.truncate()\n",
    "            filehandler.write(b'\\n')\n",
    "            filehandler.write(b'}')\n",
    "\n",
    "    searcher.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Find required number(specified by sentence_number) of most relevant sentences with the claim\n",
    "sentence processed by simple similarity\n",
    "sim_matrix are methods those take one pair of sentences as input. e.g.similarity_spacy_large\n",
    "\n",
    "'''\n",
    "def search_sentence_simple(claim, transformer, topN, parser, searcher, sim_matrix, sentence_number):\n",
    "    query_str_list = transformer(claim)\n",
    "    \n",
    "    if len(query_str_list) == 0:\n",
    "        value = {\"claim\":claim, \"label\":\"NOT ENOUGH INFO\", \"evidence\":[]}\n",
    "        return value\n",
    "    \n",
    "    # query_str_list to query_str\n",
    "    query_str = \"\\'\"+query_str_list[0]+\"\\'\"        \n",
    "    for i in range(len(query_str_list)-1):\n",
    "        query_str += \" OR \\'\"+ query_str_list[i+1] +\"\\'\"\n",
    "        \n",
    "    query = parser.parse(query_str)\n",
    "    evidence = []\n",
    "    \n",
    "    #search with collector\n",
    "    my_collector = searcher.collector(limit=topN,optimize=False)\n",
    "    tlc = TimeLimitCollector(my_collector, timelimit=30)\n",
    "    try:\n",
    "        searcher.search_with_collector(query,tlc)\n",
    "    except TimeLimit:\n",
    "        pass\n",
    "    results=tlc.results()\n",
    "    \n",
    "    sim_counter = Counter()\n",
    "    for i in range(min(topN,len(results))):\n",
    "        doc = results[i]['content']\n",
    "        sentence_list = doc.split(\"\\n\")\n",
    "        \n",
    "        for line in sentence_list:\n",
    "            if line != \"\":\n",
    "                line = line.split(\" \")\n",
    "                identifier = line[0]\n",
    "                pagenumber = int(line[1])\n",
    "                sentence = \" \".join(line[2:])\n",
    "                #print(sentence)\n",
    "                #print(claim)\n",
    "                score = sim_matrix(sentence,claim)\n",
    "                sim_counter.update({(nfd(identifier),pagenumber):score})\n",
    "                \n",
    "    for sentence_tuple,score in sim_counter.most_common(sentence_number):\n",
    "            evidence.append(list(sentence_tuple))\n",
    "\n",
    "    value = {\"claim\":claim, \"label\":\"SUPPORTS\", \"evidence\":evidence}\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Find required number(specified by sentence_number) of most relevant sentences with the claim\n",
    "sentence processed by complex similarity(tf-idf)\n",
    "sim_matrix are methods those take a set of sentences and a claim as input. e.g.gensim_lsi\n",
    "\n",
    "'''\n",
    "def search_sentence_complex(claim, transformer, topN, parser, searcher, sim_matrix, sentence_number):\n",
    "    query_str_list = transformer(claim)\n",
    "    if len(query_str_list) == 0:\n",
    "        value = {\"claim\":claim, \"label\":\"NOT ENOUGH INFO\", \"evidence\":[]}\n",
    "        return value\n",
    "    \n",
    "    # query_str_list to query_str\n",
    "    query_str = \"\\'\"+query_str_list[0]+\"\\'\"        \n",
    "    for i in range(len(query_str_list)-1):\n",
    "        query_str += \" OR \\'\"+ query_str_list[i+1] +\"\\'\"\n",
    "        \n",
    "    query = parser.parse(query_str)\n",
    "    evidence = []\n",
    "    \n",
    "    #search with collector\n",
    "    my_collector = searcher.collector(limit=topN,optimize=False)\n",
    "    tlc = TimeLimitCollector(my_collector, timelimit=30)\n",
    "    try:\n",
    "        searcher.search_with_collector(query,tlc)\n",
    "    except TimeLimit:\n",
    "        pass\n",
    "    results=tlc.results()\n",
    "    \n",
    "    sentence_dict = {}\n",
    "    whole_sentence_list = []\n",
    "    count = 0\n",
    "    for i in range(min(topN,len(results))):\n",
    "        doc = results[i]['content']\n",
    "        sentence_list = doc.split(\"\\n\")\n",
    "        \n",
    "        for line in sentence_list:\n",
    "            if line != \"\":\n",
    "                line = line.split(\" \")\n",
    "                identifier = line[0]\n",
    "                pagenumber = int(line[1])\n",
    "                sentence = \" \".join(line[2:])\n",
    "                \n",
    "                sentence_dict[count] = [nfd(identifier), pagenumber]\n",
    "                whole_sentence_list.append(sentence)\n",
    "                count += 1\n",
    "    \n",
    "    if len(whole_sentence_list) == 0:\n",
    "        return {\"claim\":claim, \"label\":\"NOT ENOUGH INFO\", \"evidence\":[]}\n",
    "        \n",
    "    sequence = sim_matrix(claim, whole_sentence_list)\n",
    "    for j in range(min(len(sequence),sentence_number)):\n",
    "        evidence.append(sentence_dict[sequence[j]])\n",
    "\n",
    "    value = {\"claim\":claim, \"label\":\"SUPPORTS\", \"evidence\":evidence}\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function that calls the above two methods to process a file.\n",
    "Output is the sentences searched by the similarity matrix.\n",
    "'''\n",
    "\n",
    "def find_sentence(openfile, savefile, transformer, topN, sim_type, sim_matrix, sentence_number):\n",
    "    ix = open_dir(\"indexdir\")\n",
    "    searcher = ix.searcher(weighting=scoring.BM25F(B=0.75, K1=1.2))\n",
    "    parser = QueryParser(None, ix.schema) \n",
    "    parser.add_plugin(qparser.MultifieldPlugin([\"title_remove_underline\"]))\n",
    "    \n",
    "    with open(openfile,'r') as f:\n",
    "        with open(savefile,\"w\",encoding='ascii') as dump_f:\n",
    "            dump_f.write('{')\n",
    "            load_dict = json.load(f)\n",
    "            for key,val in load_dict.items():\n",
    "                claim_number = key\n",
    "                claim = val['claim']\n",
    "\n",
    "                #search by claim\n",
    "                value = sim_type(claim, transformer, topN, parser, searcher, sim_matrix, sentence_number)\n",
    "\n",
    "                dump_f.write(json.dumps(claim_number)+':'+json.dumps(value))\n",
    "                dump_f.write(',')\n",
    "                dump_f.write('\\n')\n",
    "\n",
    "        with open(savefile,\"rb+\") as filehandler:\n",
    "            filehandler.seek(-3, os.SEEK_END)\n",
    "            filehandler.truncate()\n",
    "            filehandler.write(b'\\n')\n",
    "            filehandler.write(b'}')\n",
    "\n",
    "    searcher.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For training the data, create train.tsv and evaluation.tsv for the model 1 (first layer) used in BERT\n",
    "'''\n",
    "\n",
    "import csv\n",
    "def create_evidence_train_tsv(train_file,tsv_file,transformer, topN, sim_matrix, sentence_number):\n",
    "    ix = open_dir(\"indexdir\")\n",
    "    searcher = ix.searcher(weighting=scoring.BM25F(B=0.75, K1=1.2))\n",
    "    parser = QueryParser(None, ix.schema) \n",
    "    parser.add_plugin(qparser.MultifieldPlugin([\"title_remove_underline\"]))\n",
    "    \n",
    "    with open(train_file,'r') as f:\n",
    "        with open(tsv_file,\"w\",newline='',encoding='utf-8') as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            load_dict = json.load(f)\n",
    "            for key,val in load_dict.items():\n",
    "                claim_number = key\n",
    "                claim = val['claim']\n",
    "                true_label = val['label']\n",
    "                true_evidence = val['evidence']\n",
    "                \n",
    "                query_str_list = transformer(claim)\n",
    "\n",
    "                if len(query_str_list) == 0:\n",
    "                    continue\n",
    "\n",
    "                # query_str_list to query_str\n",
    "                query_str = \"\\'\"+query_str_list[0]+\"\\'\"\n",
    "                for i in range(len(query_str_list)-1):\n",
    "                    query_str += \" OR \\'\"+ query_str_list[i+1] +\"\\'\"\n",
    "\n",
    "                query = parser.parse(query_str)\n",
    "\n",
    "                #search with collector\n",
    "                my_collector = searcher.collector(limit=topN,optimize=False)\n",
    "                tlc = TimeLimitCollector(my_collector, timelimit=30)\n",
    "                try:\n",
    "                    searcher.search_with_collector(query,tlc)\n",
    "                except TimeLimit:\n",
    "                    pass\n",
    "                results=tlc.results()\n",
    "\n",
    "                sim_counter = Counter()\n",
    "                for i in range(min(topN,len(results))):\n",
    "                    doc = results[i]['content']\n",
    "                    sentence_list = doc.split(\"\\n\")\n",
    "\n",
    "                    for line in sentence_list:\n",
    "                        if line != \"\":\n",
    "                            line = line.split(\" \")\n",
    "                            identifier = line[0]\n",
    "                            pagenumber = int(line[1])\n",
    "                            sentence = \" \".join(line[2:])\n",
    "                            score = sim_matrix(sentence,claim)\n",
    "                            sim_counter.update({(nfd(identifier),pagenumber,sentence):score})\n",
    "\n",
    "                for sentence_tuple,score in sim_counter.most_common(sentence_number):\n",
    "                        if [sentence_tuple[0],sentence_tuple[1]] in true_evidence:\n",
    "                            tsv_writer.writerow([claim_number,sentence_tuple[0],sentence_tuple[1],'yes', claim, sentence_tuple[2]])\n",
    "                        else:\n",
    "                            tsv_writer.writerow([claim_number,sentence_tuple[0],sentence_tuple[1],'no', claim, sentence_tuple[2]])\n",
    "    searcher.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function used to create training file for second layer(label judgment).\n",
    "tsv_file_1 includes all the claims by \"SUPPORTS\" and \"REFUTES\" labels.\n",
    "tsv_file_2 includes all the claims by \"NOT ENOUGH INFO\" labels.\n",
    "'''\n",
    "def create_label_train_tsv(train_file,tsv_file_1,tsv_file_2,transformer, topN, sim_matrix, sentence_number):\n",
    "    ix = open_dir(\"indexdir\")\n",
    "    searcher = ix.searcher(weighting=scoring.BM25F(B=0.75, K1=1.2))\n",
    "    parser = QueryParser(None, ix.schema) \n",
    "    parser.add_plugin(qparser.MultifieldPlugin([\"title_remove_underline\"]))\n",
    "    parser2 = QueryParser(None, ix.schema)\n",
    "    parser2.add_plugin(qparser.MultifieldPlugin([\"title\"]))\n",
    "    \n",
    "    with open(train_file,'r') as f:\n",
    "        with open(tsv_file_1,\"w\",newline='',encoding='utf-8') as out_file_1:\n",
    "            with open(tsv_file_2,\"w\",newline='',encoding='utf-8') as out_file_2:\n",
    "                tsv_writer_1 = csv.writer(out_file_1, delimiter='\\t')\n",
    "                tsv_writer_2 = csv.writer(out_file_2, delimiter='\\t')\n",
    "                load_dict = json.load(f)\n",
    "                for key,val in load_dict.items():\n",
    "                    claim_number = key\n",
    "                    claim = val['claim']\n",
    "                    true_label = val['label']\n",
    "                    true_evidence = val['evidence']\n",
    "\n",
    "                    if true_label == \"SUPPORTS\" or true_label == \"REFUTES\":\n",
    "                        sentence_combination = \"\"\n",
    "                        for one_evidence in true_evidence:\n",
    "                            query_str = nfc(one_evidence[0])\n",
    "                            query_pagenumber = one_evidence[1]\n",
    "                            query = parser2.parse(query_str)\n",
    "\n",
    "                            #search with collector\n",
    "                            my_collector = searcher.collector(limit=1,optimize=False)\n",
    "                            tlc = TimeLimitCollector(my_collector, timelimit=30)\n",
    "                            try:\n",
    "                                searcher.search_with_collector(query,tlc)\n",
    "                            except TimeLimit:\n",
    "                                pass\n",
    "                            results=tlc.results()\n",
    "\n",
    "                            for i in range(min(1,len(results))):\n",
    "                                doc = results[i]['content']\n",
    "                                sentence_list = doc.split(\"\\n\")\n",
    "\n",
    "                                for line in sentence_list:\n",
    "                                    if line != \"\":\n",
    "                                        line = line.split(\" \")\n",
    "                                        identifier = line[0]\n",
    "                                        pagenumber = int(line[1])\n",
    "                                        sentence = \" \".join(line[2:])\n",
    "                                        if pagenumber == query_pagenumber:\n",
    "                                            sentence_combination = sentence_combination+\" \"+sentence\n",
    "                                            break\n",
    "                        tsv_writer_1.writerow([claim_number,true_label, claim, sentence_combination])\n",
    "\n",
    "                    #the label is \"NOT ENOUGH INFO\"\n",
    "                    else: \n",
    "                        query_str_list = transformer(claim)\n",
    "\n",
    "                        if len(query_str_list) == 0:\n",
    "                            continue\n",
    "\n",
    "                        # query_str_list to query_str\n",
    "                        query_str = \"\\'\"+query_str_list[0]+\"\\'\"\n",
    "                        for i in range(len(query_str_list)-1):\n",
    "                            query_str += \" OR \\'\"+ query_str_list[i+1] +\"\\'\"\n",
    "\n",
    "                        query = parser.parse(query_str)\n",
    "\n",
    "                        #search with collector\n",
    "                        my_collector = searcher.collector(limit=topN,optimize=False)\n",
    "                        tlc = TimeLimitCollector(my_collector, timelimit=30)\n",
    "                        try:\n",
    "                            searcher.search_with_collector(query,tlc)\n",
    "                        except TimeLimit:\n",
    "                            pass\n",
    "                        results=tlc.results()\n",
    "\n",
    "                        sim_counter = Counter()\n",
    "                        for i in range(min(topN,len(results))):\n",
    "                            doc = results[i]['content']\n",
    "                            sentence_list = doc.split(\"\\n\")\n",
    "\n",
    "                            for line in sentence_list:\n",
    "                                if line != \"\":\n",
    "                                    line = line.split(\" \")\n",
    "                                    identifier = line[0]\n",
    "                                    pagenumber = int(line[1])\n",
    "                                    sentence = \" \".join(line[2:])\n",
    "                                    score = sim_matrix(sentence,claim)\n",
    "                                    sim_counter.update({(nfd(identifier),pagenumber,sentence):score})\n",
    "\n",
    "                        for sentence_tuple,score in sim_counter.most_common(sentence_number):\n",
    "                            tsv_writer_2.writerow([claim_number,sentence_tuple[0],sentence_tuple[1],'unknown', claim, sentence_tuple[2]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Process tsv_file_2 in last method, filter 20 sentences by model 1 for each claim\n",
    "probility_file is the output of model 1 and input undone_file is tsv_file_2\n",
    "prediction_file should be combined with tsv_file_1 as the input file for the second layer training in BERT(model 2)\n",
    "'''\n",
    "import csv\n",
    "def create_no_enough_info_training_set(undone_file,probility_file,predition_file):\n",
    "    with open(undone_file,'r',encoding='utf-8') as tsvfile_1:\n",
    "        with open(probility_file,'r') as tsvfile_2:\n",
    "            with open(predition_file,'w',newline='',encoding='utf-8') as tsvfile_3:\n",
    "                tsv_reader_1 = csv.reader(tsvfile_1, delimiter=\"\\t\")\n",
    "                tsv_reader_2 = csv.reader(tsvfile_2, delimiter=\"\\t\")\n",
    "                tsv_writer = csv.writer(tsvfile_3, delimiter=\"\\t\")\n",
    "                \n",
    "                claim_number = \"\"\n",
    "                claim = \"\"\n",
    "                current_sentences = \"\"\n",
    "                for undone_sentence,prob_sentence in zip(tsv_reader_1, tsv_reader_2):\n",
    "                    if float(prob_sentence[0]) >= 0.5:\n",
    "                        if undone_sentence[0] == claim_number: #append sentence\n",
    "                            current_sentences = current_sentences + \" \" + undone_sentence[5]\n",
    "                        elif claim_number != \"\": #output and reset claim number\n",
    "                            tsv_writer.writerow([claim_number,\"NOT ENOUGH INFO\", claim, current_sentences])\n",
    "                            claim_number = undone_sentence[0]\n",
    "                            claim = undone_sentence[4]\n",
    "                            current_sentences = undone_sentence[5]\n",
    "                        else: #initial the first one\n",
    "                            claim_number = undone_sentence[0]\n",
    "                            claim = undone_sentence[4]\n",
    "                            current_sentences = undone_sentence[5]\n",
    "                tsv_writer.writerow([claim_number,\"NOT ENOUGH INFO\", claim, current_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The function that process the test json file in the first step.\n",
    "tsv_file_step1 is the input of the model 1, that can filter 20 sentences to evidence\n",
    "json_file_step1 is a json file that includes those who have not response documents in the Whoosh\n",
    "'''\n",
    "\n",
    "def process_test_step1(test_file,tsv_file_step1,json_file_step1,transformer, topN, sim_matrix, sentence_number):\n",
    "    ix = open_dir(\"indexdir\")\n",
    "    searcher = ix.searcher(weighting=scoring.BM25F(B=0.75, K1=1.2))\n",
    "    parser = QueryParser(None, ix.schema) \n",
    "    parser.add_plugin(qparser.MultifieldPlugin([\"title_remove_underline\"]))\n",
    "    \n",
    "    with open(test_file,'r') as f:\n",
    "        with open(tsv_file_step1,\"w\",newline='',encoding='utf-8') as out_file:\n",
    "            with open(json_file_step1,\"w\") as jsonfile:\n",
    "                tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "                load_dict = json.load(f)\n",
    "                new_dict = {}\n",
    "                for key,val in load_dict.items():\n",
    "                    claim_number = key\n",
    "                    claim = val['claim']\n",
    "\n",
    "                    query_str_list = transformer(claim)\n",
    "\n",
    "                    if len(query_str_list) == 0:\n",
    "                        new_dict[claim_number] = {\"claim\":claim, 'label':\"NOT ENOUGH INFO\", \"evidence\":[]}\n",
    "                        continue\n",
    "\n",
    "                    # query_str_list to query_str\n",
    "                    query_str = \"\\'\"+query_str_list[0]+\"\\'\"\n",
    "                    for i in range(len(query_str_list)-1):\n",
    "                        query_str += \" OR \\'\"+ query_str_list[i+1] +\"\\'\"\n",
    "\n",
    "                    query = parser.parse(query_str)\n",
    "\n",
    "                    #search with collector\n",
    "                    my_collector = searcher.collector(limit=topN,optimize=False)\n",
    "                    tlc = TimeLimitCollector(my_collector, timelimit=30)\n",
    "                    try:\n",
    "                        searcher.search_with_collector(query,tlc)\n",
    "                    except TimeLimit:\n",
    "                        pass\n",
    "                    results=tlc.results()\n",
    "                    \n",
    "                    if len(results) == 0:\n",
    "                        new_dict[claim_number] = {\"claim\":claim, 'label':\"NOT ENOUGH INFO\", \"evidence\":[]}\n",
    "                        continue\n",
    "\n",
    "                    sim_counter = Counter()\n",
    "                    for i in range(min(topN,len(results))):\n",
    "                        doc = results[i]['content']\n",
    "                        sentence_list = doc.split(\"\\n\")\n",
    "\n",
    "                        for line in sentence_list:\n",
    "                            if line != \"\":\n",
    "                                line = line.split(\" \")\n",
    "                                identifier = line[0]\n",
    "                                pagenumber = int(line[1])\n",
    "                                sentence = \" \".join(line[2:])\n",
    "                                score = sim_matrix(sentence,claim)\n",
    "                                sim_counter.update({(nfd(identifier),pagenumber,sentence):score})\n",
    "\n",
    "                    for sentence_tuple,score in sim_counter.most_common(sentence_number):\n",
    "                        tsv_writer.writerow([claim_number,sentence_tuple[0],sentence_tuple[1],'unknown', claim, sentence_tuple[2]])\n",
    "                json.dump(new_dict,jsonfile,indent=2)\n",
    "    searcher.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The function that process the output of the step1 and model 1\n",
    "prediction_of_step1 is the output tsv file of tsv_file_step1 through model 1\n",
    "json_file_step2 is the output json file which records all the evidence, but no label.\n",
    "'''\n",
    "\n",
    "def process_test_step2(tsv_file_step1,json_file_step1,prediction_of_step1,tsv_file_step2, json_file_step2):\n",
    "    with open(tsv_file_step1,'r',encoding='utf-8') as t_1, open(json_file_step1,'r') as j_1:\n",
    "        with open(prediction_of_step1,'r') as p:\n",
    "            with open(tsv_file_step2, 'w',newline='',encoding='utf-8') as t_2, open(json_file_step2, 'w', encoding='ascii') as j_2:\n",
    "                tsv_reader_1 = csv.reader(t_1, delimiter=\"\\t\")\n",
    "                tsv_reader_p = csv.reader(p, delimiter=\"\\t\")\n",
    "                tsv_writer = csv.writer(t_2, delimiter=\"\\t\")\n",
    "                load_dict = json.load(j_1)\n",
    "                                \n",
    "                claim_number = \"\"\n",
    "                claim = \"\"\n",
    "                current_sentences = \"\"\n",
    "                current_evidence = []\n",
    "                for unfiltered_sentence, prob_sentence in zip(tsv_reader_1, tsv_reader_p):\n",
    "                    if unfiltered_sentence[0] == claim_number: #in the same claim\n",
    "                        if float(prob_sentence[0]) >= 0.5:#append sentences set\n",
    "                            current_sentences = current_sentences + \" \" + unfiltered_sentence[5]\n",
    "                            current_evidence.append([nfd(unfiltered_sentence[1]),int(unfiltered_sentence[2])])\n",
    "                    elif claim_number != \"\": #output last claim\n",
    "                        if current_sentences == \"\": #no evidence\n",
    "                            load_dict[claim_number] = {\"claim\":claim, 'label':\"NOT ENOUGH INFO\", \"evidence\":[]}\n",
    "                            #update the claim info\n",
    "                            claim_number = unfiltered_sentence[0]\n",
    "                            claim = unfiltered_sentence[4]\n",
    "                            if float(prob_sentence[0]) >= 0.5:\n",
    "                                current_sentences = unfiltered_sentence[5]\n",
    "                                current_evidence = [[nfd(unfiltered_sentence[1]),int(unfiltered_sentence[2])]]\n",
    "                        else: #has evidence\n",
    "                            load_dict[claim_number] = {\"claim\":claim, 'label':\"SUPPORTS\", \"evidence\":current_evidence}\n",
    "                            tsv_writer.writerow([claim_number,\"unknown\", claim, current_sentences])\n",
    "                            #update the claim info\n",
    "                            claim_number = unfiltered_sentence[0]\n",
    "                            claim = unfiltered_sentence[4]\n",
    "                            if float(prob_sentence[0]) >= 0.5:\n",
    "                                current_sentences = unfiltered_sentence[5]\n",
    "                                current_evidence = [[nfd(unfiltered_sentence[1]),int(unfiltered_sentence[2])]]\n",
    "                            else:\n",
    "                                current_sentences = \"\"\n",
    "                                current_evidence = []\n",
    "                    else: #initialize\n",
    "                        claim_number = unfiltered_sentence[0]\n",
    "                        claim = unfiltered_sentence[4]\n",
    "                        if float(prob_sentence[0]) >= 0.5:\n",
    "                            current_sentences = unfiltered_sentence[5]\n",
    "                            current_evidence = [[nfd(unfiltered_sentence[1]),int(unfiltered_sentence[2])]]\n",
    "                            \n",
    "                # process the last claim\n",
    "                if current_sentences == \"\":\n",
    "                    load_dict[claim_number] = {\"claim\":claim, 'label':\"NOT ENOUGH INFO\", \"evidence\":[]}\n",
    "                else:\n",
    "                    load_dict[claim_number] = {\"claim\":claim, 'label':\"SUPPORTS\", \"evidence\":current_evidence}\n",
    "                    tsv_writer.writerow([claim_number,\"unknown\", claim, current_sentences])\n",
    "                \n",
    "                #new judgement after sentence filter\n",
    "                json.dump(load_dict,j_2,indent=2)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The function that process the output of the step2 and model 2\n",
    "prediction_of_step2 is the output tsv file of tsv_file_step2 through model 2\n",
    "json_file_step3 is the final output json file which records all the evidence, and labels.\n",
    "'''\n",
    "def process_test_step3(tsv_file_step2, json_file_step2, prediction_of_step2, json_file_step3):\n",
    "    with open(tsv_file_step2,'r',encoding='utf-8') as t_2, open(json_file_step2,'r') as j_2:\n",
    "        with open(prediction_of_step2,'r') as p, open(json_file_step3, 'w', encoding='ascii') as j_3:\n",
    "            tsv_reader_1 = csv.reader(t_2, delimiter=\"\\t\")\n",
    "            tsv_reader_p = csv.reader(p, delimiter=\"\\t\")\n",
    "            load_dict = json.load(j_2)\n",
    "            \n",
    "            for filtered_sentence, prob_sentence in zip(tsv_reader_1, tsv_reader_p):\n",
    "                claim_number = filtered_sentence[0]\n",
    "                claim = filtered_sentence[2]\n",
    "                label = \"\"\n",
    "                if float(prob_sentence[2]) >= max(float(prob_sentence[0]),float(prob_sentence[1])):\n",
    "                    label = \"NOT ENOUGH INFO\"\n",
    "                    load_dict[claim_number] = {\"claim\":claim, 'label':label, \"evidence\":[]}\n",
    "                else:\n",
    "                    if float(prob_sentence[0]) > float(prob_sentence[1]):\n",
    "                        label = \"SUPPORTS\"\n",
    "                    else:\n",
    "                        label = \"REFUTES\"\n",
    "                    load_dict[claim_number]['label'] = label\n",
    "                    \n",
    "            #new judgement after sentence filter\n",
    "            json.dump(load_dict,j_3,indent=2)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A test for binary classification in model 2\n",
    "'''\n",
    "def process_test_step3_binary(tsv_file_step2, json_file_step2, prediction_of_step2, json_file_step3):\n",
    "    with open(tsv_file_step2,'r',encoding='utf-8') as t_2, open(json_file_step2,'r') as j_2:\n",
    "        with open(prediction_of_step2,'r') as p, open(json_file_step3, 'w', encoding='ascii') as j_3:\n",
    "            tsv_reader_1 = csv.reader(t_2, delimiter=\"\\t\")\n",
    "            tsv_reader_p = csv.reader(p, delimiter=\"\\t\")\n",
    "            load_dict = json.load(j_2)\n",
    "            \n",
    "            for filtered_sentence, prob_sentence in zip(tsv_reader_1, tsv_reader_p):\n",
    "                claim_number = filtered_sentence[0]\n",
    "                claim = filtered_sentence[2]\n",
    "                label = \"\"\n",
    "                \n",
    "                if float(prob_sentence[0]) > float(prob_sentence[1]):\n",
    "                    label = \"SUPPORTS\"\n",
    "                else:\n",
    "                    label = \"REFUTES\"\n",
    "                load_dict[claim_number]['label'] = label\n",
    "                    \n",
    "            #new judgement after sentence filter\n",
    "            json.dump(load_dict,j_3,indent=2)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A dumb test for 'NEI' portion check in test file. For error analysis\n",
    "'''\n",
    "\n",
    "import json\n",
    "def dumb_test_for_NEI(raw_json,dumb_json):\n",
    "    with open(raw_json,'r') as j_1, open(dumb_json, 'w', encoding='ascii') as j_2:\n",
    "        load_dict = json.load(j_1)\n",
    "        for claim_number in load_dict:\n",
    "            load_dict[claim_number]['label'] = 'NOT ENOUGH INFO'\n",
    "            load_dict[claim_number]['evidence'] = []\n",
    "            \n",
    "        json.dump(load_dict,j_2,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_test_for_NEI('test-unlabelled.json','test-dumb-test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Below is the main process of training set.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 4122.98531627655 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "create_evidence_train_tsv(\"devset2000.json\",\"devset2000.tsv\",spacy_pos_single,10,similarity_spacy_large,20)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 8682.938073396683 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "create_label_train_tsv(\"train15000.json\",\"train15000-1.tsv\",\"train15000-2.tsv\",spacy_pos_single,10,similarity_spacy_large,20)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 6336.4470064640045 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "create_evidence_train_tsv(\"devset2001-5001.json\",\"devset2001-5001.tsv\",spacy_pos_single,10,similarity_spacy_large,20)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 9246.978297710419 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "create_label_train_tsv(\"train35000-50000.json\",\"train35000-50000-1.tsv\",\"train35000-50000-2.tsv\",spacy_pos_single,10,similarity_spacy_large,20)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Below is the main process of test set.(After model 1 and 2 are built in BERT)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 33634.59951758385 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "process_test_step1(\"test-unlabelled.json\",\"final-test-step1.tsv\",\"testoutput-1.json\", \\\n",
    "                   spacy_pos_single, 10, similarity_spacy_large, 20)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 3.9154000282287598 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "process_test_step2(\"final-test-step1.tsv\",\"testoutput-1.json\",\"final-test-step1-prob.tsv\",\"final-test-step2.tsv\", \"testoutput-2.json\")\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 0.6721642017364502 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "process_test_step3(\"final-test-step2.tsv\", \"testoutput-2.json\", \"final-test-step2-prob.tsv\", \"testoutput-3.json\")\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 0.8081531524658203 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "process_test_step3_binary(\"final-test-step2.tsv\", \"testoutput-2.json\", \"final-test-step2-prob-binary.tsv\", \"testoutput-3-binary.json\")\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Below are some document-recall tests in the early stage.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 993.3926341533661 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "find_doc(\"train5000.json\",\"train5000-pos-single-20-1.2-noascii.json\", spacy_pos_single, 20)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 917.1151354312897 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "find_doc(\"devset.json\",\"devset-pos-single-10.json\", spacy_pos_single, 10)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 6458.375021457672 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "find_doc(\"devset.json\",\"devset-pos-single-10-combination.json\", spacy_pos_single, 10)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 5078.554181814194 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "find_doc(\"devset.json\",\"devset-pos-single-10-content.json\", spacy_pos_single, 10)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 1301.2530605793 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "find_doc(\"devset.json\",\"devset-chunk-10.json\", spacy_chunk, 10)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 625.510498046875 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "find_doc(\"devset.json\",\"devset-ner-10.json\", spacy_ner, 10)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 832.1719839572906 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "find_doc(\"devset.json\",\"devset-pos-single-5.json\", spacy_pos_single, 5)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 969.7718818187714 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "find_doc(\"devset.json\",\"devset-pos-single-20.json\", spacy_pos_single, 20)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 1867.9652078151703 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "find_doc(\"devset.json\",\"devset-pos-final-10.json\", spacy_pos_final, 10)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 1977.4455802440643 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "find_doc(\"devset.json\",\"devset-chunk-single-10.json\", spacy_chunk_single, 10)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Below are some sentences-recall tests in the early stage\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 1186.3261201381683 s\n"
     ]
    }
   ],
   "source": [
    "#genism TF-IDF\n",
    "time_start=time.time()\n",
    "find_sentence(\"train5000.json\",\"train5000-sentence-gensim_lsi.json\", spacy_pos_single,10,\\\n",
    "              search_sentence_complex, gensim_lsi, 20)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start=time.time()\n",
    "find_sentence(\"train5000.json\",\"train5000-sentence-spacy.json\", spacy_pos_single,10,\\\n",
    "              search_sentence_simple, similarity_spacy_large, 20)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start=time.time()\n",
    "find_sentence(\"train5000.json\",\"train5000-sentence-vector.json\", spacy_pos_single,10,\\\n",
    "              search_sentence_simple, vector_similarity, 20)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start=time.time()\n",
    "find_sentence(\"train5000.json\",\"train5000-sentence-cosine.json\", spacy_pos_single,10,\\\n",
    "              search_sentence_simple, cosine_similarity, 20)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start=time.time()\n",
    "find_sentence(\"test-unlabelled.json\",\"test-sentence-spacy.json\", spacy_pos_single,10,\\\n",
    "              search_sentence_simple, similarity_spacy_large, 20)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
